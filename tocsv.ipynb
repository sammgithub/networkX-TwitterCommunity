{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q pandas==1.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result['includes']['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t = result['includes']['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = result['includes']['users']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = result['data']\n",
    "# r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"referenced_tweets\" in r:\n",
    "    for items in r['referenced_tweets']:\n",
    "        print(items['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get():\n",
    "    if 'referenced_tweets' in r:\n",
    "        print('data are there')\n",
    "        for i,j in enumerate(r):\n",
    "            print(i+1, j['referenced_tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,x in enumerate(r):\n",
    "# #     for p,q in enumerate(x):\n",
    "# #         print(p+1,q[0]['author_id'])\n",
    "    \n",
    "#     print(x['referenced_tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in r:\n",
    "#     print(i['referenced_tweets'][0]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['includes']['users'][0]['public_metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r[0]['id']#.explode(ignore_index=True)#column='rt_ref_tweet_id',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(r),len(t),len(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in r:\n",
    "#     print(i['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,x in enumerate(r):\n",
    "#     print(i+1,x['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,x in enumerate(result['includes']['tweets']):\n",
    "#     print(i+1,x)               # tweet['text']) #['created_at'], tweet['id'],tweet['author_id']\n",
    "#     print(x['created_at'], x['id'],x['author_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a,b,c = 0,0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class jsonLister():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def t_Parse(self, tweets):\n",
    "        retval = {\n",
    "            \"t_created_at\":tweets['created_at'],\n",
    "            \"t_id\":tweets['id'],\n",
    "            \"t_author_id\":tweets['author_id'],\n",
    "            \"t_text\":tweets['text'],\n",
    "#             \"t_lang\":tweets['lang'],\n",
    "            ## '''no <for> loop is implemented as public_metrics is a dictionary'''\n",
    "            \"t_rt_count\":tweets['public_metrics']['retweet_count'],\n",
    "            \"t_rp_count\":tweets['public_metrics']['reply_count'],\n",
    "            \"t_lk_count\":tweets['public_metrics']['like_count'],\n",
    "            \"t_qt_count\":tweets['public_metrics']['quote_count']\n",
    "#             \"annotations_start\":[],\n",
    "#             \"annotations_end\":[],\n",
    "#             \"url\":[],\n",
    "#             \"display_url\":[],\n",
    "            }\n",
    "        return retval\n",
    "    \n",
    "    def rt_Parse(self, data):\n",
    "        retval = {\n",
    "            \"rt_created_at\":data['created_at'],\n",
    "            \"rt_author_id\":data['author_id'],\n",
    "            \"rt_lang\":data['lang'],\n",
    "            \"rt_text\":data['text'],\n",
    "            \"rt_id\":data['id'],\n",
    "#             \"rt_convr_id\":data['conversation_id'],\n",
    "            \"rt_ref_tweet_id\":[],\n",
    "            \"rt_ref_tweet_type\":[],\n",
    "#             \"rt_annotations_start\":[],\n",
    "#             \"rt_annotations_end\":[],\n",
    "#             \"rt_url\":[],\n",
    "#             \"rt_display_url\":[],\n",
    "             ##'''no <for> loop is implemented as public_metrics is a dictionary'''\n",
    "            \"rt_rt_count\":data['public_metrics']['retweet_count'],\n",
    "            \"rt_rp_count\":data['public_metrics']['reply_count'],\n",
    "            \"rt_lk_count\":data['public_metrics']['like_count'],\n",
    "            \"rt_qt_count\":data['public_metrics']['quote_count']\n",
    "            }\n",
    "        \n",
    "        if \"referenced_tweets\" in data:\n",
    "            for items in data['referenced_tweets']:\n",
    "                retval['rt_ref_tweet_id'].append(items['id'])\n",
    "                retval['rt_ref_tweet_type'].append(items['type'])\n",
    "\n",
    "#         if \"entities\" in dataItem:\n",
    "#             if \"annotations\" in dataItem['entities']:\n",
    "#                 for annotation in dataItem['entities']['annotations']:\n",
    "#                     if \"start\" in annotation:\n",
    "#                         retval['rt_annotations_start'].append(annotation['start'])\n",
    "#                         retval['rt_annotations_end'].append(annotation['end'])\n",
    "\n",
    "#             if \"urls\" in dataItem['entities']:\n",
    "#                 for url in dataItem['entities']['urls']:\n",
    "#                     if \"url\" in url:\n",
    "#                         retval['rt_url'].append(url['url'])\n",
    "#                     if \"display_url\" in url:\n",
    "#                         retval['rt_display_url'].append(url['display_url'])\n",
    "            \n",
    "        return retval\n",
    "    \n",
    "    ## Only required when working with <user>    \n",
    "    def user_Parse(self, user):\n",
    "    ## '''this block can take data manually like in other methods, however the following dict trick gets them directly as it is in dict'''\n",
    "#         retval = {\n",
    "#             \"username\":user['username'],\n",
    "#             \"username\":user['id'],\n",
    "#             \"user_tweet_count\":users['public_metrics']['tweet_count'],\n",
    "#             \"author_id\":dataItem['author_id'],    \n",
    "#             }\n",
    "\n",
    "        if \"public_metrics\" in user:\n",
    "            for k, v in user['public_metrics'].items():\n",
    "                user[k] = v\n",
    "            del user['public_metrics']\n",
    "        return user\n",
    "\n",
    "    def parser(self, result):\n",
    "        t_parsed_list = []\n",
    "        # loop to work on 'tweets'\n",
    "        for tweet in result['includes']['tweets']:\n",
    "            element = self.t_Parse(tweet)\n",
    "            t_parsed_list.append(element)\n",
    "        t_parsed_list = pd.DataFrame(t_parsed_list)    \n",
    "#         t_parsed_list.to_csv('tweets.csv', encoding='utf-8', index=False)\n",
    "        \n",
    "        rt_parsed_list = []\n",
    "        # loop to work on 'data list'\n",
    "        for item in result['data']:\n",
    "            element = self.rt_Parse(item)\n",
    "            rt_parsed_list.append(element)\n",
    "        ## used <explode()> to create new rows for each id\n",
    "        rt_parsed_list = pd.DataFrame(rt_parsed_list).explode(column='rt_ref_tweet_id',ignore_index=True)\n",
    "        \n",
    "    ## only required when working with <user> fields\n",
    "        user_parsed_list = []\n",
    "        # loop to work on 'users'\n",
    "        for user in result['includes']['users']:\n",
    "            element = self.user_Parse(user)\n",
    "            user_parsed_list.append(element)\n",
    "        user_parsed_list = pd.DataFrame(user_parsed_list)\n",
    "#         user_parsed_list.to_csv('users.csv', encoding='utf-8', index=False) # \n",
    "\n",
    "        global a, b, c\n",
    "        a,b,c = rt_parsed_list, t_parsed_list, user_parsed_list\n",
    "        ## Merge the tables now based on ids' <author_id, id> etc.\n",
    "        table = rt_parsed_list.merge(t_parsed_list, left_on='rt_ref_tweet_id', right_on='t_id', how='left')\n",
    "#         table[['rt_id','rt_author_id','rt_ref_tweet_id','t_id','rt_author_id']].to_csv('test5_jan.csv', encoding='utf-8', index=False) ## this is for testing only\n",
    "        \n",
    "        \n",
    "        table = table.merge(user_parsed_list, left_on='rt_author_id', right_on='id', how='left')\n",
    "        \n",
    "#         table.to_csv('test_jan.csv', encoding='utf-8', index=False) ## this is for testing only\n",
    "#         table[['rt_author_id','t_author_id','t_rt_count']].to_csv('test3_jan.txt', encoding='utf-8', index=False) ## this is for testing only\n",
    "        \n",
    "        \n",
    "#         table.rt_ref_tweet_type = table.rt_ref_tweet_type.apply(','.join)\n",
    "        table.rt_created_at= pd.to_datetime(table.rt_created_at)\n",
    "        table.t_created_at= pd.to_datetime(table.t_created_at)\n",
    "        table.created_at= pd.to_datetime(table.created_at)\n",
    "#         table.to_csv('dec19_01-31_000000-235959.csv', encoding='utf-8', index=False)\n",
    "        \n",
    "        ## Other options [optional]\n",
    "#         table.rt_created_at= table.rt_created_at.apply(lambda x: x.split('.')[0].replace('T',' '))\n",
    "#         table = table.merge(user_parsed_list, how=\"left\", left_on='tt_author_id', right_on='id')\n",
    "#         table = table.drop_duplicates(\"tt_text\", keep='first')\n",
    "            \n",
    "#         print(t_parsed_list)\n",
    "#         print(\"-\"*100)\n",
    "#         print(rt_parsed_list)\n",
    "#         print(\"=\"*100)\n",
    "#         print(user_parsed_list)\n",
    "        \n",
    "        ### see customized prints to find match to do the merging    \n",
    "#         print(t_parsed_list[['t_id','t_author_id']])\n",
    "#         print(rt_parsed_list[['rt_ref_tweet_id','rt_id']])        #,'rt_author_id'\n",
    "#         print(user_parsed_list[['id','followers_count', 'following_count']])\n",
    "       \n",
    "        return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jl = jsonLister()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('may20_04-07_000000-055959.jsonl', 'r', encoding=\"utf8\") as file:\n",
    "    res = []\n",
    "    for json_str in file:\n",
    "        result = json.loads(json_str)\n",
    "        res.append(jl.parser(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(res, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('may20_04-07_000000-055959.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_w = df[df[\"rt_created_at\"].between('2019-12-01 00:00:00','2019-12-01 23:59:59')]#df[(df['rt_created_at']> \"2019-12-01\") & (df['rt_created_at']< \"2019-12-03\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uw = df[['rt_author_id','t_author_id','rt_created_at']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uw.to_csv('dec_uw_1x24.tsv',index=False, sep='\\t') #.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_rt = df[['t_author_id','rt_author_id','t_rt_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uw.plot(x ='t_author_id', y='rt_created_at', kind = 'line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"rt_created_at\"].between('2020-02-05 00:00:01','2020-02-05 07:59:59')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_fl = df[['t_author_id','rt_author_id','followers_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=pd.read_csv('dec19_01-31_000000-235959.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_3d = d[d[\"rt_created_at\"].between('2019-12-01 00:00:00','2019-12-03 23:59:59')]#df[(df['rt_created_at']> \"2019-12-01\") & (df['rt_created_at']< \"2019-12-03\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_d = d_3d[['rt_author_id','t_author_id','t_created_at']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uw = d_d[d_d['t_author_id'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uw = df_uw[['rt_author_id','t_author_id']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uw.to_csv('dec_uw_3d.csv',index=False, sep=',') #.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = a.reset_index()\n",
    "# c.rt_ref_tweet_type = c.rt_ref_tweet_type.apply(','.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max(a.rt_ref_tweet_type.apply(lambda x: 0 if x == np.NaN else len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a[a.rt_ref_tweet_type.apply(lambda x: 0 if x == np.NaN else len(x)) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c[a.rt_ref_tweet_type.apply(lambda x: 0 if x == np.NaN else len(x)) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.rt_ref_tweet_type = c.rt_ref_tweet_type.apply(','.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('rt_tt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\" \n",
    "Pandas dataframe.nunique() function return Series with number of distinct observations over requested axis. \n",
    "If we set the value of axis to be 0, then it finds the total number of unique observations over the index axis. \n",
    "If we set the value of axis to be 1, then it find the total number of unique observations over the column axis. \n",
    "It also provides the feature to exclude the NaN values from the count of unique numbers.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.nunique(axis=0, dropna=True)a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = len(df) - df.nunique()\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop_duplicates(\"tt_text\", keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1['rt_ref_tweet_id'] = df1['rt_ref_tweet_id'].str.strip('['']') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['rt_ref_tweet_id'] = df2['rt_ref_tweet_id'].str.replace('\\W', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1['rt_ref_tweet_id'] = df1['rt_ref_tweet_id'].str.replace('\\W', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"data\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in data['data'][0]['entities']['annotations']:\n",
    "    print(x['start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data['data']:\n",
    "    print(item['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data['data'][0]['referenced_tweets']:\n",
    "    print(item['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dummy.jsonl', 'r', encoding=\"utf8\") as file:\n",
    "  for json_str in file:\n",
    "    data = json.loads(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findKey(result, path): #path= [a,b,c]\n",
    "  pointer = result\n",
    "  for i in range(len(path)):\n",
    "    if pointer is not None and path[i] in pointer:\n",
    "      pointer = pointer[path[i]]\n",
    "    else:\n",
    "      #print(path[i], \"not in\", \"->\".join([\"result\", *path[:i]]))\n",
    "      print(\"no data\")\n",
    "      return None\n",
    "  return pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = []\n",
    "with open('dummy.jsonl', 'r', encoding=\"utf8\") as file:\n",
    "  for json_str in file:\n",
    "    result = json.loads(json_str)\n",
    "    row = []\n",
    "    #print(findKey(result, [\"entities\",\"urls\",\"indices\"])); \n",
    "    #print(findKey(result, [\"user\",\"id_str\"])); #print(findKey(result, [\"user\", \"id\"])); #print(findKey(result, [\"user\", \"name\"]))\n",
    "\n",
    "    ## Re-tweets >> get all the retweets information\n",
    "    row.append(findKey(result, [\"data\",\"created_at\"]))\n",
    "#     row.append(findKey(result, [\"data\",\"entities\",\"mentions\",\"username\"]))\n",
    "#     row.append(findKey(result, [\"data\",\"entities\",\"hashtags\",\"tag\"]))    \n",
    "#     row.append(findKey(result, [\"data\",\"lang\"]))\n",
    "#     row.append(findKey(result, [\"data\",\"conversation_id\"]))\n",
    "    row.append(findKey(result, [\"data\",\"author_id\"]))    \n",
    "#     row.append(findKey(result, [\"data\",\"public_metrics\",\"retweet_count\"]))    \n",
    "#     row.append(findKey(result, [\"data\",\"public_metrics\",\"reply_count\"]))    \n",
    "#     row.append(findKey(result, [\"data\",\"public_metrics\",\"like_count\"]))    \n",
    "#     row.append(findKey(result, [\"data\",\"public_metrics\",\"quote_count\"\"]))\n",
    "    \n",
    "    ## Tweets >> get all the tweets information    \n",
    "    row.append(findKey(result, [\"includes\",\"tweets\", \"created_at\"]))\n",
    "    row.append(findKey(result, [\"includes\",\"tweets\", \"lang\"]))\n",
    "    row.append(findKey(result, [\"includes\",\"tweets\", \"text\"]))\n",
    "#     row.append(findKey(result, [\"tweets\", \"conversation_id\"]))\n",
    "#     row.append(findKey(result, [\"tweets\", \"entities\",\"hashtags\",\"tag\"]))\n",
    "#     row.append(findKey(result, [\"tweets\", \"author_id\"]))\n",
    "#     row.append(findKey(result, [\"tweets\",\"public_metrics\",\"retweet_count\"]))    \n",
    "#     row.append(findKey(result, [\"tweets\",\"public_metrics\",\"reply_count\"]))    \n",
    "#     row.append(findKey(result, [\"tweets\",\"public_metrics\",\"like_count\"]))    \n",
    "#     row.append(findKey(result, [\"tweets\",\"public_metrics\",\"quote_count\"\"]))\n",
    "                             \n",
    "    table.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df = pd.DataFrame(table, columns =[\n",
    "    \"rt_created_at\",\n",
    "    \"rt_user_id\",\n",
    "#     \"rt_language\",\n",
    "#     \"retweet_text\",\n",
    "\n",
    "#             \"user_mentions_screen_name\",\n",
    "#             \"user_mentions_name\",\n",
    "#             \"user_mentions_id\",\n",
    "            \n",
    "#             \"user_id\",\n",
    "#             \"user_name\",\n",
    "#             \"user_screen_name\",\n",
    "#             \"user_location\",\n",
    "#             \"user_description\",\n",
    "#             \"user_followers_count\",\n",
    "#             \"user_friends_count\",\n",
    "#             \"user_listed_count\",\n",
    "#             \"user_created_at\",\n",
    "#             \"user_favourites_count\",\n",
    "#             \"user_statuses_count\",\n",
    "\n",
    "    \"t_created_at\",\n",
    "    \"lang\",\n",
    "    \"text\"\n",
    "#             \"rt_id\",\n",
    "#             \"rt_text\",\n",
    "#             \"t_author_id\",\n",
    "#             \"rt_user_name\",\n",
    "#             \"rt_user_screen_name\",\n",
    "#             \"rt_user_location\",\n",
    "#             \"rt_user_followers_count\",\n",
    "#             \"rt_user_friends_count\",\n",
    "#             \"rt_user_listed_count\",\n",
    "#             \"rt_user_created_at\",\n",
    "#             \"rt_user_favourites_count\",\n",
    "#             \"rt_user_statuses_count\"\n",
    "#             \"rt_user_retweet_count\",\n",
    "#             \"rt_user_favorite_count\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a ='hello'\n",
    "b ='hi'\n",
    "c ='ok'*9\n",
    "x = [a,b,c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in enumerate(x):\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2020-01-31T23:56:20.000Z\n",
    "2020-01-31 23:56:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('jan20_02-02_000000-235959.jsonl', 'r', encoding=\"utf8\") as file:\n",
    "    res = []\n",
    "    for json_str in file:\n",
    "        result = json.loads(json_str)\n",
    "        res.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res[0]['includes']['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter=0\n",
    "for p in res:\n",
    "    for j in p['data']:\n",
    "        if 'referenced_tweets' in j:\n",
    "            counter +=1\n",
    "            print(counter, j['referenced_tweets'][0]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j['referenced_tweets'][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0]['includes']['tweets'][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0]['data'][0]['referenced_tweets'][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('file.tsv',index=False, sep='\\t') #.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# subprocess.call(\"jupyter nbconvert notebook.ipynb --to pdf\")\n",
    "# Kind of a hacky way, but it will work. If you're using Windows, you may need to add the additional kwarg shell=True to subprocess.call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subprocess.call doesn't accept string. You should pass subprocess.call(shlex.split(\"jupyter nbconvert notebook.ipynb --to pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
